{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(word):\n",
    "    word = unicode_to_ascii(word.lower().strip())\n",
    "    word = re.sub(r\"([?.!,])\", r\" \\1 \", word)\n",
    "    word = re.sub(r'[\" \"]+', \" \", word)\n",
    "    word = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", word)\n",
    "    word = word.rstrip().strip()\n",
    "    word = '<start> ' + word + ' <end>'\n",
    "    return word\n",
    "\n",
    "def create_dataset(filename, nums):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    line = [line.strip().split('\\n') for line in lines]\n",
    "    word_pairs = [[preprocess_sentence(s) for s in l[0].split('\\t')] for l in line[:nums]]\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(ts):\n",
    "    return max(len(t) for t in ts)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_data(numbers_example):\n",
    "    dataset = create_dataset('fra.txt', numbers_example)\n",
    "    input_lang = [token[0] for token in dataset]\n",
    "    target_lang = [token[1] for token in dataset]\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(input_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(target_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_example = 35000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_data(numbers_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into training and testing or use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 28000 7000 7000\n"
     ]
    }
   ],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_test), len(target_tensor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "# convert(inp_lang, input_tensor_train[0])\n",
    "# convert(targ_lang, target_tensor_train[0])\n",
    "# print (\"Input Language; index to word mapping\")\n",
    "# print (\"Target Language; index to word mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_tensor_train)\n",
    "batch_size = 64\n",
    "step_per_epoch = buffer_size//batch_size\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_input_size = len(inp_lang.index_word)+1\n",
    "vocab_target_size = len(targ_lang.index_word)+1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 10]), TensorShape([64, 17]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((np.random.uniform(size=(10, 6)), np.random.uniform(size=(10, 4)))).shuffle(2)\n",
    "# dataset = dataset.batch(3, drop_remainder=True)\n",
    "# a = next(iter(dataset))\n",
    "# print(a)\n",
    "# # print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode_model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sze):\n",
    "        super(encode_model, self).__init__()\n",
    "        self.batch_sze = batch_sze\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sze, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 10, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = encode_model(vocab_input_size, embedding_dim, units, batch_size)\n",
    "# sample test\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder.call(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        score = self.V(tf.nn.tanh(self.W1(values)+ self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights*values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of attention output context vectors: (batch size, units) (64, 1024)\n",
      "shape of attention weights: (batch size, sequence length) (64, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = attention(10)\n",
    "attention_context_vector, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "print('shape of attention output context vectors: (batch size, units) {}'.format(attention_context_vector.shape))\n",
    "print('shape of attention weights: (batch size, sequence length) {}'.format(attention_weights.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decode_model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sze):\n",
    "        super(decode_model, self).__init__()\n",
    "        self.batch_sze = batch_sze\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = attention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 8292)\n"
     ]
    }
   ],
   "source": [
    "decoder = decode_model(vocab_target_size, embedding_dim, units, batch_size)\n",
    "sample_decode_output, _, _ = decoder(tf.random.uniform((64,1)), sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decode_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(truth, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(truth, 0))\n",
    "    loss_ = loss_object(truth, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = './training_checkpoint_final'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(Input, target, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(Input, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "        # This is the step of teacher forcing\n",
    "        for t in range(1, target.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(target[: ,t], 1)\n",
    "        \n",
    "    batch_loss = loss / int(target.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 Batch0 Loss3.4833\n",
      "Epoch1 Batch200 Loss1.4531\n",
      "Epoch1 Batch400 Loss1.2163\n",
      "Epoch1 Loss1.4585\n",
      "Time1495.8422219753265 sec\n",
      "\n",
      "Epoch2 Batch0 Loss1.0732\n",
      "Epoch2 Batch200 Loss1.0037\n",
      "Epoch2 Batch400 Loss0.8849\n",
      "Epoch2 Loss0.9494\n",
      "Time1493.738127231598 sec\n",
      "\n",
      "Epoch3 Batch0 Loss0.7647\n",
      "Epoch3 Batch200 Loss0.7326\n",
      "Epoch3 Batch400 Loss0.6728\n",
      "Epoch3 Loss0.6925\n",
      "Time1494.3516147136688 sec\n",
      "\n",
      "Epoch4 Batch0 Loss0.4185\n",
      "Epoch4 Batch200 Loss0.5127\n",
      "Epoch4 Batch400 Loss0.5343\n",
      "Epoch4 Loss0.4907\n",
      "Time1493.9165074825287 sec\n",
      "\n",
      "Epoch5 Batch0 Loss0.3406\n",
      "Epoch5 Batch200 Loss0.3634\n",
      "Epoch5 Batch400 Loss0.3991\n",
      "Epoch5 Loss0.3516\n",
      "Time1492.1430087089539 sec\n",
      "\n",
      "Epoch6 Batch0 Loss0.2194\n",
      "Epoch6 Batch200 Loss0.2284\n",
      "Epoch6 Batch400 Loss0.2983\n",
      "Epoch6 Loss0.2625\n",
      "Time1499.0714755058289 sec\n",
      "\n",
      "Epoch7 Batch0 Loss0.1594\n",
      "Epoch7 Batch200 Loss0.1786\n",
      "Epoch7 Batch400 Loss0.2510\n",
      "Epoch7 Loss0.2071\n",
      "Time1454.8906807899475 sec\n",
      "\n",
      "Epoch8 Batch0 Loss0.1273\n",
      "Epoch8 Batch200 Loss0.1500\n",
      "Epoch8 Batch400 Loss0.2096\n",
      "Epoch8 Loss0.1712\n",
      "Time1453.1325697898865 sec\n",
      "\n",
      "Epoch9 Batch0 Loss0.1109\n",
      "Epoch9 Batch200 Loss0.1332\n",
      "Epoch9 Batch400 Loss0.1607\n",
      "Epoch9 Loss0.1486\n",
      "Time1500.166847229004 sec\n",
      "\n",
      "Epoch10 Batch0 Loss0.0973\n",
      "Epoch10 Batch200 Loss0.1388\n",
      "Epoch10 Batch400 Loss0.1280\n",
      "Epoch10 Loss0.1325\n",
      "Time1502.543048620224 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    for (batch_index, (Input, target)) in enumerate(dataset.take(step_per_epoch)):\n",
    "        batch_loss = train_step(Input, target, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch_index % 200 == 0:\n",
    "            print('Epoch{} Batch{} Loss{:.4f}'.format(epoch+1, batch_index, batch_loss.numpy()))\n",
    "    \n",
    "    if (epoch+1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    print('Epoch{} Loss{:.4f}'.format(epoch+1, total_loss/step_per_epoch))\n",
    "    print('Time{} sec\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5fb2c6e710>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a set of sentences from the testing data, and print the translation results from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2word(lang, tensor):\n",
    "    sentence_list = ''\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            sentence_list += lang.index_word[t] + \" \"\n",
    "    return sentence_list\n",
    "\n",
    "def get_sentence(indexs):\n",
    "    Inpout = tensor2word(inp_lang, input_tensor_test[indexs])\n",
    "    true_result = tensor2word(targ_lang, target_tensor_test[indexs])\n",
    "    sentence = Inpout[7:-6]\n",
    "    return sentence, true_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i m surviving . <end>\n",
      "Predicted translation: je survis . <end> \n"
     ]
    }
   ],
   "source": [
    "sentences, _ = get_sentence(12)\n",
    "translate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> they re cool . <end>\n",
      "Predicted translation: ils sont sympa . <end> \n"
     ]
    }
   ],
   "source": [
    "sentences, _ = get_sentence(13)\n",
    "translate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> you ve worked hard . <end>\n",
      "Predicted translation: vous avez travaille d arrache pied . <end> \n"
     ]
    }
   ],
   "source": [
    "sentences, _ = get_sentence(14)\n",
    "translate(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the BLEU score for the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def get_bleu(index):\n",
    "    sentences, true_result = get_sentence(index)\n",
    "    prediction, _, _ = evaluate(sentences)\n",
    "    \n",
    "    prediction = prediction.split(' ')[:-3]\n",
    "    true_result = [true_result.split(' ')[1:-3]]\n",
    "    bleu_score = sentence_bleu(true_result, prediction, weights=(1, 0, 0, 0))\n",
    "    print('Prediction sentence: %s' % (prediction))\n",
    "    print('Truth ground sentence: %s' % (true_result))\n",
    "    print('BLEU score: %f' % bleu_score)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction sentence: ['c', 'est', 'un', 'mauvais', 'garcon']\n",
      "Truth ground sentence: [['c', 'est', 'un', 'mauvais', 'garcon']]\n",
      "BLEU score: 1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bleu(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_bleu(index):\n",
    "    sentences, true_result = get_sentence(index)\n",
    "    prediction, _, _ = evaluate(sentences)\n",
    "    prediction = prediction.split(' ')[:-3]\n",
    "    true_result = [true_result.split(' ')[1:-3]]\n",
    "    bleu_score = sentence_bleu(true_result, prediction, weights=(1, 0, 0, 0))\n",
    "    return bleu_score\n",
    "\n",
    "bleu_score_list = []\n",
    "length = len(input_tensor_test)\n",
    "\n",
    "for i in range(1000):\n",
    "    bleu_score_list.append(get_only_bleu(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for the testing dataset: 0.761948\n"
     ]
    }
   ],
   "source": [
    "average_mean = np.mean(bleu_score_list)\n",
    "print('BLEU score for the testing dataset: %f' % average_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
